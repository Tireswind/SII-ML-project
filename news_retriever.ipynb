{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser as fp\n",
    "import json\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from time import mktime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the limit for number of articles to download\n",
    "LIMIT = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['newspapers'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the JSON files with news sites\n",
    "with open('C:\\\\Users\\\\alessandra.flaccaven\\\\saved_files\\\\NewsPapers.json') as data_file:\n",
    "    companies = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading articles from  newyorktimes_business\n",
      "10 articles downloaded from newyorktimes_business , url:  https://www.nytimes.com/2020/02/12/business/dealbook/bernie-sanders-business.html?emc=rss&partner=rss\n",
      "20 articles downloaded from newyorktimes_business , url:  https://www.nytimes.com/2020/02/11/business/under-armour-earnings.html?emc=rss&partner=rss\n",
      "30 articles downloaded from newyorktimes_business , url:  https://www.nytimes.com/2020/02/10/business/recycling-law.html?emc=rss&partner=rss\n",
      "40 articles downloaded from newyorktimes_business , url:  https://www.nytimes.com/2020/02/10/us/politics/equifax-hack-china.html?emc=rss&partner=rss\n",
      "50 articles downloaded from newyorktimes_business , url:  https://www.nytimes.com/2020/02/09/technology/amazon-bookstore-nazis.html?emc=rss&partner=rss\n",
      "Downloading articles from  newyorktimes_science\n",
      "10 articles downloaded from newyorktimes_science , url:  https://www.nytimes.com/2020/02/12/climate/blackrock-oil-sands-alberta-financing.html?emc=rss&partner=rss\n",
      "20 articles downloaded from newyorktimes_science , url:  https://www.nytimes.com/2020/02/10/health/alzheimers-amyloid-drug.html?emc=rss&partner=rss\n",
      "30 articles downloaded from newyorktimes_science , url:  https://www.nytimes.com/2020/02/08/world/asia/xi-coronavirus-china.html?emc=rss&partner=rss\n",
      "40 articles downloaded from newyorktimes_science , url:  https://www.nytimes.com/2020/02/07/world/asia/coronavirus-china.html?emc=rss&partner=rss\n",
      "Downloading articles from  newyorktimes_technology\n",
      "10 articles downloaded from newyorktimes_technology , url:  https://www.nytimes.com/2020/02/10/us/politics/equifax-hack-china.html?emc=rss&partner=rss\n",
      "20 articles downloaded from newyorktimes_technology , url:  https://www.nytimes.com/2020/02/07/business/clearview-facial-recognition-child-sexual-abuse.html?emc=rss&partner=rss\n",
      "Downloading articles from  cnnmoney\n",
      "Downloading articles from  marketwatch_newsletters\n",
      "Downloading articles from  marketwatch_stories\n",
      "10 articles downloaded from marketwatch_stories , url:  http://www.marketwatch.com/news/story.asp?guid=%7B8E422604-4D7B-11EA-88EE-61EF8C6DD7F2%7D&siteid=rss&rss=1\n",
      "Downloading articles from  cnbc_top_news\n",
      "10 articles downloaded from cnbc_top_news , url:  https://www.cnbc.com/2020/02/12/trader-sees-a-comeback-for-apple-to-all-time-highs.html\n",
      "20 articles downloaded from cnbc_top_news , url:  https://www.cnbc.com/2020/02/12/how-to-avoid-catching-coronavirus-on-an-airplane.html\n",
      "30 articles downloaded from cnbc_top_news , url:  https://www.cnbc.com/2020/02/12/the-ecb-and-fed-are-hoping-the-public-will-give-them-a-new-direction-central-banks.html\n",
      "Downloading articles from  cnbc_investing\n",
      "10 articles downloaded from cnbc_investing , url:  https://www.cnbc.com/2020/02/07/boeing-wells-fargo-among-stocks-put-in-penalty-box-by-investors.html\n",
      "20 articles downloaded from cnbc_investing , url:  https://www.cnbc.com/2020/02/05/stodgy-utilities-competing-with-tech-stocks-for-hottest-sector-in-the-market-right-now.html\n",
      "30 articles downloaded from cnbc_investing , url:  https://www.cnbc.com/2020/02/04/ron-baron-touts-penn-national-investment-in-barstool-sports.html\n",
      "Downloading articles from  marketwatch\n",
      "10 articles downloaded from marketwatch , url:  https://www.theguardian.com/politics/2020/feb/12/british-nationals-working-in-brussels-snap-up-irish-passports\n",
      "20 articles downloaded from marketwatch , url:  https://www.theguardian.com/news/audio/2020/feb/12/the-former-addicts-saving-lives-with-naloxone-podcast\n",
      "30 articles downloaded from marketwatch , url:  https://www.theguardian.com/sport/2020/feb/12/f1-chinese-gp-set-to-be-called-off-due-to-coronavirus-with-vietnam-race-at-risk\n",
      "40 articles downloaded from marketwatch , url:  https://www.theguardian.com/uk-news/video/2020/jan/21/why-stronger-borders-dont-work\n",
      "50 articles downloaded from marketwatch , url:  https://www.theguardian.com/uk-news/2020/feb/12/uk-army-years-off-full-strength-despite-recruitment-drive\n",
      "Downloading articles from  fox_business_latest_headlines\n",
      "10 articles downloaded from fox_business_latest_headlines , url:  https://www.foxnews.com/lifestyle/disneyland-raises-ticket-prices-again-unveils-5-tier-system\n",
      "Downloading articles from  fox_business_opinion\n",
      "10 articles downloaded from fox_business_opinion , url:  http://feeds.foxnews.com/~r/foxnews/opinion/~3/DTZ1DDHMyiw/trump-democrats-executive-excess-andrew-mccarthy\n",
      "20 articles downloaded from fox_business_opinion , url:  http://feeds.foxnews.com/~r/foxnews/opinion/~3/RkrVYNw3WLU/miranda-devine-petulant-nancy-pelosi-is-everything-wrong-with-democratic-party\n",
      "Downloading articles from  entrepreneur_latest\n",
      "10 articles downloaded from entrepreneur_latest , url:  http://feedproxy.google.com/~r/entrepreneur/latest/~3/UVifQ5U9NSc/346211\n",
      "Downloading articles from  entrepreneur_marketing\n",
      "10 articles downloaded from entrepreneur_marketing , url:  http://feedproxy.google.com/~r/entrepreneur/salesandmarketing/~3/TFk6_hzkof4/345785\n",
      "Downloading articles from  reuters_money\n",
      "10 articles downloaded from reuters_money , url:  http://feeds.reuters.com/~r/news/wealth/~3/v3jK9v9EtsA/activist-hedge-funds-stepped-up-calls-for-asset-sales-and-spin-offs-in-2019-data-idUSKBN1ZE1TT\n",
      "20 articles downloaded from reuters_money , url:  http://feeds.reuters.com/~r/news/wealth/~3/EoZPXao44yw/u-s-retirement-legislation-aims-to-improve-workplace-saving-income-options-idUSKBN1YN29T\n",
      "Downloading articles from  reuters_science\n",
      "10 articles downloaded from reuters_science , url:  http://feeds.reuters.com/~r/reuters/scienceNews/~3/zPEzf2TDkLs/nasa-astronaut-koch-returns-to-earth-after-record-space-mission-idUSKBN20012Y\n",
      "20 articles downloaded from reuters_science , url:  http://feeds.reuters.com/~r/reuters/scienceNews/~3/HIJysA6ppcw/australia-scientists-to-share-lab-grown-coronavirus-to-hasten-vaccine-efforts-idUSKBN1ZR2YD\n",
      "Downloading articles from  sciencedaily\n",
      "10 articles downloaded from sciencedaily , url:  https://www.sciencedaily.com/releases/2020/02/200211134538.htm\n",
      "20 articles downloaded from sciencedaily , url:  https://www.sciencedaily.com/releases/2020/02/200211103833.htm\n",
      "30 articles downloaded from sciencedaily , url:  https://www.sciencedaily.com/releases/2020/02/200211092550.htm\n",
      "40 articles downloaded from sciencedaily , url:  https://www.sciencedaily.com/releases/2020/02/200210153326.htm\n",
      "50 articles downloaded from sciencedaily , url:  https://www.sciencedaily.com/releases/2020/02/200210133213.htm\n",
      "Downloading articles from  economist_business\n",
      "Downloading articles from  economist_science_and_tech\n",
      "Downloading articles from  fivethirtyeight_science\n",
      "10 articles downloaded from fivethirtyeight_science , url:  https://fivethirtyeight.com/features/its-gettin-hot-in-here-so-be-a-sawtail-fish/\n",
      "20 articles downloaded from fivethirtyeight_science , url:  https://fivethirtyeight.com/features/the-world-isnt-ready-for-climate-refugees/\n",
      "Downloading articles from  fivethirtyeight_economics\n",
      "10 articles downloaded from fivethirtyeight_economics , url:  https://fivethirtyeight.com/features/what-if-tariffs-cost-trump-the-farm-vote/\n",
      "20 articles downloaded from fivethirtyeight_economics , url:  https://fivethirtyeight.com/features/trumps-position-on-iran-shows-how-much-harder-north-korea-will-be/\n",
      "Downloading articles from  nbpostgazette\n",
      "10 articles downloaded from nbpostgazette , url:  https://nbpostgazette.com/who-is-qasem-soleimani-and-why-was-he-killed-why-is-world-war-3/\n",
      "Downloading articles from  journalismday\n",
      "10 articles downloaded from journalismday , url:  https://journalismday.com/2017/07/global-microtomes-market-2/\n",
      "Downloading articles from  satprnews\n",
      "Downloading articles from  insidertradings\n",
      "Downloading articles from  truthfulreporter\n",
      "Downloading articles from  highlandmirror\n",
      "Downloading articles from  thefinancialanalyst\n",
      "Downloading articles from  reportagestuff\n",
      "Downloading articles from  tokenfolks\n",
      "Downloading articles from  heraldanalyst\n",
      "Downloading articles from  findmarketresearch\n",
      "saving articles . . . in scraped_articles.json\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "\n",
    "# Iterate through each news company\n",
    "# the company is the name, the value is the dictionary of links\n",
    "for company, value in companies.items():\n",
    "    # If a RSS link is provided in the JSON file, this will be the first choice, \n",
    "    # as RSS feeds often give more consistent and correct data\n",
    "    if 'rss' in value:\n",
    "        d = fp.parse(value['rss'])\n",
    "        print(\"Downloading articles from \", company)\n",
    "        newsPaper = {\n",
    "            \"rss\": value['rss'],\n",
    "            \"link\": value['link'],\n",
    "            \"articles\": []\n",
    "        }\n",
    "        for entry in d.entries:\n",
    "            # Check if publish date is provided, if not, the article is skipped.\n",
    "            # This is done to keep consistency in the data and to keep the script from crashing.\n",
    "            if hasattr(entry, 'published'):\n",
    "                if count > LIMIT:\n",
    "                    break\n",
    "                article = {}\n",
    "                article['link'] = entry.link\n",
    "                date = entry.published_parsed\n",
    "                article['published'] = datetime.fromtimestamp(mktime(date)).isoformat()\n",
    "                try:\n",
    "                    content = Article(entry.link)\n",
    "                    content.download()\n",
    "                    content.parse()\n",
    "                except Exception as e:\n",
    "                    # If the download for some reason fails (ex. 404) the script will continue downloading\n",
    "                    # the next article.\n",
    "                    print(e)\n",
    "                    print(\"continuing...\")\n",
    "                    continue\n",
    "                article['title'] = content.title\n",
    "                article['text'] = content.text\n",
    "                article['author'] = content.authors\n",
    "                newsPaper['articles'].append(article)\n",
    "                if count % 10 == 0:\n",
    "                    print(count, \"articles downloaded from\", company, \", url: \", entry.link)\n",
    "                count = count + 1\n",
    "    else:\n",
    "        # if a RSS-feed link is not provided\n",
    "        # the python newspaper library will be used to extract articles\n",
    "        print(\"Building site for \", company)\n",
    "        paper = newspaper.build(value['link'], memoize_articles=False)\n",
    "        newsPaper = {\n",
    "            \"link\": value['link'],\n",
    "            \"articles\": []\n",
    "        }\n",
    "        noneTypeCount = 0\n",
    "        for content in paper.articles:\n",
    "            if count > LIMIT:\n",
    "                break\n",
    "            try:\n",
    "                content.download()\n",
    "                content.parse()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"continuing...\")\n",
    "                continue\n",
    "            # Again, for consistency, if there is no found publish date the article will be skipped.\n",
    "            # After 10 downloaded articles from the same newspaper without publish date, the company will be skipped.\n",
    "            if content.publish_date is None:\n",
    "                print(count, \" Article has date of type None...\")\n",
    "                noneTypeCount = noneTypeCount + 1\n",
    "                if noneTypeCount > 10:\n",
    "                    print(\"Too many noneType dates, aborting...\")\n",
    "                    noneTypeCount = 0\n",
    "                    break\n",
    "                count = count + 1\n",
    "                continue\n",
    "            article = {}\n",
    "            article['title'] = content.title\n",
    "            article['text'] = content.text\n",
    "            article['link'] = content.url\n",
    "            article['published'] = content.publish_date.isoformat()\n",
    "            article['author'] = content.authors\n",
    "            newsPaper['articles'].append(article)\n",
    "            if count % 10 == 0: \n",
    "                print(count, \"articles downloaded from\", company, \" using newspaper, url: \", content.url)\n",
    "            count = count + 1\n",
    "            noneTypeCount = 0\n",
    "    count = 1\n",
    "    data['newspapers'][company] = newsPaper\n",
    "\n",
    "\n",
    "# Finally it saves the articles as a JSON-file.\n",
    "try:\n",
    "    fname = 'scraped_articles.json'\n",
    "    print('saving articles . . . in {}'.format(fname))\n",
    "    with open(fname, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
